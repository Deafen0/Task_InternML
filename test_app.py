import streamlit as st
import torch
from transformers import pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from docx import Document
from pypdf import PdfReader
from groq import Groq
import time

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–†–ê–ù–ò–¶–´ ---
st.set_page_config(page_title="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –°–∞–º–º–∞—Ä–∏", page_icon="üìÑ", layout="wide")

# --- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô ---
@st.cache_resource
def load_local_model(language="English"):
    if language == "–†—É—Å—Å–∫–∏–π":
        model_name = "cointegrated/rut5-base-multitask"
    else:
        model_name = "facebook/bart-large-cnn"
    
    st.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {model_name}...")
    try:
        task = "text2text-generation" if "t5" in model_name else "summarization"
        summarizer = pipeline(task, model=model_name)
        st.success("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        return summarizer
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ '{model_name}': {e}")
        return None

# --- –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò ---
def summarize_with_local_model(text_to_summarize, summarizer, min_length=50, max_length=200):
    if not summarizer:
        return "–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."
    summary_list = None # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    try:
        is_t5 = "t5" in summarizer.model.name_or_path
        if is_t5:
            text_to_summarize = "summarize: " + text_to_summarize
        
        summary_list = summarizer(text_to_summarize, max_length=max_length, min_length=min_length, do_sample=False)
        
        if is_t5:
            return summary_list[0]['generated_text']
        else:
            return summary_list[0]['summary_text']
            
    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        if summary_list:
            print(f"–í—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–∑–≤–∞–ª –æ—à–∏–±–∫—É: {summary_list}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}"

def summarize_with_groq(text_to_summarize, groq_client, model_name, language):
    if language == "–†—É—Å—Å–∫–∏–π":
        prompt = f"""
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–¥–µ–ª–∞—Ç—å –∫—Ä–∞—Ç–∫—É—é, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—É—é –≤—ã–∂–∏–º–∫—É (—Å–∞–º–º–∞—Ä–∏) –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        –°–∞–º–º–∞—Ä–∏ –¥–æ–ª–∂–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏, —Ñ–∞–∫—Ç—ã –∏ –≤—ã–≤–æ–¥—ã. –°–æ—Ö—Ä–∞–Ω—è–π –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π —Ç–æ–Ω.
        –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:
        ---
        {text_to_summarize}
        ---
        –ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ (—Å–∞–º–º–∞—Ä–∏):
        """
    else: # English prompt
        prompt = f"""
        Your task is to create a concise yet comprehensive summary of the provided text in English.
        The summary should reflect the key ideas, facts, and conclusions. Maintain a neutral and informative tone.
        Text to process:
        ---
        {text_to_summarize}
        ---
        Concise summary:
        """
    try:
        chat_completion = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}], model=model_name)
        return chat_completion.choices[0].message.content
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Groq API: {e}"

def map_reduce_summarization(text, model_choice, language, api_key=None, local_summarizer=None):
    if model_choice == 'Mistral (Groq API)':
        if not api_key:
            st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à Groq API –∫–ª—é—á.")
            return None
        chunk_size, chunk_overlap = 8000, 400
        client = Groq(api_key=api_key)
        model_name = "llama3-70b-8192"
        summarize_func = lambda chunk: summarize_with_groq(chunk, client, model_name, language)
    else:
        if not local_summarizer:
            st.error("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
            return None
        chunk_size, chunk_overlap = 2000, 200
        summarize_func = lambda chunk: summarize_with_local_model(chunk, local_summarizer)

    if len(text) < chunk_size * 1.2:
        st.info("–¢–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏ –Ω–∞–ø—Ä—è–º—É—é...")
        return summarize_func(text)
    st.info("–¢–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è Map-Reduce...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)
    chunks = text_splitter.split_text(text)
    st.write(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–∞–º–º–∞—Ä–∏...")
    initial_summaries = []
    progress_bar = st.progress(0)
    for i, chunk in enumerate(chunks):
        summary = summarize_func(chunk)
        initial_summaries.append(summary)
        time.sleep(0.1)
        progress_bar.progress((i + 1) / len(chunks))
    st.write("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∞–º–º–∞—Ä–∏...")
    combined_summary = "\n\n".join(initial_summaries)
    if len(combined_summary) > chunk_size * 1.2:
        st.warning("–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–∞–º–º–∞—Ä–∏ –≤—Å–µ –µ—â–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ. –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–∂–∞—Ç–∏–µ...")
        final_summary = map_reduce_summarization(combined_summary, model_choice, language, api_key, local_summarizer)
    else:
        final_summary = summarize_func(combined_summary)
    return final_summary

# --- –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ß–¢–ï–ù–ò–Ø –§–ê–ô–õ–û–í ---
def read_text_file(uploaded_file): return uploaded_file.read().decode("utf-8")
def read_docx_file(uploaded_file): return "\n".join([para.text for para in Document(uploaded_file).paragraphs])
def read_pdf_file(uploaded_file): return "\n".join([page.extract_text() for page in PdfReader(uploaded_file).pages])

# --- –ò–ù–¢–ï–†–§–ï–ô–° –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ---
st.title("üìÑ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ö—Ä–∞—Ç–∫–∏—Ö –°–∞–º–º–∞—Ä–∏")
st.markdown("–í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –µ–≥–æ –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É.")
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    language = st.selectbox("1. –í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞:", ("–†—É—Å—Å–∫–∏–π", "English"))
    model_choice = st.radio("2. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", ('–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å', 'Mistral (Groq API)'))
    api_key = None
    if model_choice == 'Mistral (Groq API)':
        api_key = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à Groq API –∫–ª—é—á:", type="password")
    st.markdown("---")
    st.info("–î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥–æ–º Map-Reduce.")

input_method = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –≤–≤–æ–¥–∞:", ("–í—Å—Ç–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç", "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"), horizontal=True)
user_text = ""
if input_method == "–í—Å—Ç–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç":
    user_text = st.text_area("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à —Ç–µ–∫—Å—Ç –∑–¥–µ—Å—å:", height=300)
else:
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª (.txt, .pdf, .docx)", type=["txt", "pdf", "docx"])
    if uploaded_file:
        with st.spinner("–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞..."):
            if uploaded_file.name.endswith(".txt"): user_text = read_text_file(uploaded_file)
            elif uploaded_file.name.endswith(".docx"): user_text = read_docx_file(uploaded_file)
            elif uploaded_file.name.endswith(".pdf"): user_text = read_pdf_file(uploaded_file)
        st.success("–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")

if st.button("‚ú® –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∞–º–º–∞—Ä–∏", type="primary"):
    if not user_text.strip():
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
    else:
        local_summarizer = None
        if model_choice == '–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å':
            with st.spinner(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —è–∑—ã–∫–∞ '{language}'... (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)"):
                local_summarizer = load_local_model(language)
        
        with st.spinner("–ú–∞–≥–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ... –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏..."):
            final_summary = map_reduce_summarization(
                text=user_text,
                model_choice=model_choice,
                language=language,
                api_key=api_key,
                local_summarizer=local_summarizer
            )
        if final_summary:
            st.subheader("üéâ –í–∞—à–µ —Å–∞–º–º–∞—Ä–∏ –≥–æ—Ç–æ–≤–æ:")
            st.markdown(f"> {final_summary}")